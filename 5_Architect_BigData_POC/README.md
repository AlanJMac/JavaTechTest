# 5. Task: Architect Big Data POC

## Task Description

Assuming that you are the architect of a robust application which has a MVC architecture pattern and fetches data from a relational database, you received a requirement to create a new feature that will display a large amount of data on screen. This data is the result of combining tables in the database and it is causing a huge performance issue in the application. List at least 5 (five) solutions you would propose to do a POC explaining why each one could potentially solve the Issue.

## Responses

1. Assess if the performance issue is primarily due to viewing the data in the client application (e.g. in a browser). Consider limiting how much data is visualised at a time by implementing pagination of results or lazy-loading of components in the client, server or both.
   Reducing how many items are displayed in the client application will reduce the amount of resources the client needs to dedicate to “painting” the items and/or listening for any client events.
2. Similarly to the previous point, consider limiting how much data is requested at a time by implementing pagination or down sampling of retrieved data on the server.
   If a large amount of data is requested from the server then this could be limited by available bandwidth as well as the time/memory needed to serialise and deserialise objects in order to send them across the network. Reducing the requested data into batches can improve the perceived performance by the user. Down sampling is useful, for example, when requesting time series data for a chart, e.g. a stock ticker, as there is no point trying to display more data points than the number of pixels in the client’s monitor. Therefore, down sampling can reduce the amount of data retrieve to only the necessary amount to be visualised.
3. Assess if all the variables (e.g. database columns) requested in the query to the database are needed by the client. Additional variables means higher overhead in terms of memory and processing to serialise/deserialise objects. Requesting some variables may require additional table joins, so if these are not required by the client then the overall query may be simplified.
4. Assess if the relational database can be restructured to reduce the number of table joins needed. Each table join effectively implies an additional database query which can slow down the overall request. Denormalisation of a database could mean storing redundant information in tables but could result in overall read performance to increase as there are fewer joins to make.
5. By creating materialised views in the database for common queries it is possible to have an effective table already containing the results of all the necessary table joins. Further filtering can be made over the materialised view rather than launching the more complex query across multiple tables. This approach needs to be assessed per use case as there is an overhead in terms of memory required and the time needed to refresh the views.
6. If the database is already structured optimally, assess that relevant indexes have been created in order to speed up reading and filtering of tables. Verify that the queries being made to the database are using the indexes as and when expected. Sometime the database optimiser chooses to use an index that can actually slow down a query or, conversely, considers a sequential scan when a better index could have been used. It would be necessary to run ANALYZE over the SQL queries in various scenarios.
7. Assess if requests for the data can be parallelised and combined for the final result. By running a request across more threads/cores in parallel, the overall time may be reduced.
8. If any of the database tables are particularly large (i.e. “big data” sized), then assess if the tables can be split into appropriate smaller blocks. For example, when querying time series data, the tables could be split into years so that a request for a given time period only needs to query a smaller set of data in the relevant table. These tables can then be split across clusters for faster processing and parallelisation. There are several solutions available to handle this case, such as Apache Hadoop.
